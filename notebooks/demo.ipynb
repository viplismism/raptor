{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912cd8c6",
   "metadata": {},
   "source": [
    "# RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
    "\n",
    "This notebook demonstrates RAPTOR using **Anthropic Claude** for summarization and QA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Make sure you have a `.env` file in the project root with your API keys:\n",
    "```\n",
    "ANTHROPIC_API_KEY=sk-ant-...\n",
    "OPENAI_API_KEY=sk-...  # for embeddings\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7d995",
   "metadata": {},
   "outputs": [],
   "source": "# Load the sample document\nwith open('../demo/sample.txt', 'r') as file:\n    text = file.read()\n\nprint(text[:100])"
  },
  {
   "cell_type": "markdown",
   "id": "c7d51ebd",
   "metadata": {},
   "source": [
    "## How RAPTOR works\n",
    "\n",
    "1. **Building**: RAPTOR recursively embeds, clusters, and summarizes chunks of text to construct a tree with varying levels of summarization from the bottom up. You can create a tree from the text in `sample.txt` using `RA.add_documents(text)`.\n",
    "\n",
    "2. **Querying**: At inference time, the RAPTOR model retrieves information from this tree, integrating data across lengthy documents at different abstraction levels. You can perform queries on the tree with `RA.answer_question`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f58830",
   "metadata": {},
   "source": [
    "### Building the tree (Claude + OpenAI embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3753fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raptor import RetrievalAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e843edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default config now uses Claude for summarization & QA, OpenAI for embeddings\n",
    "RA = RetrievalAugmentation()\n",
    "\n",
    "# Build the tree\n",
    "RA.add_documents(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f219d60a",
   "metadata": {},
   "source": [
    "### Querying from the tree\n",
    "\n",
    "```python\n",
    "question = \"any question\"\n",
    "RA.answer_question(question)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4037c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How did Cinderella reach her happy ending?\"\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be7e57",
   "metadata": {},
   "outputs": [],
   "source": "# Save the tree\nSAVE_PATH = \"../demo/cinderella\"\nRA.save(SAVE_PATH)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e845de9",
   "metadata": {},
   "outputs": [],
   "source": "# Load back the tree\nRA = RetrievalAugmentation(tree=SAVE_PATH)\n\nanswer = RA.answer_question(question=question)\nprint(\"Answer:\", answer)"
  },
  {
   "cell_type": "markdown",
   "id": "277ab6ea",
   "metadata": {},
   "source": [
    "## Using custom models\n",
    "\n",
    "RAPTOR is designed to be flexible. You can use any models for summarization, QA, and embeddings by extending the base classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-sbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raptor import (\n",
    "    BaseSummarizationModel,\n",
    "    BaseQAModel,\n",
    "    BaseEmbeddingModel,\n",
    "    SBertEmbeddingModel,\n",
    "    RetrievalAugmentationConfig,\n",
    "    ClaudeSummarizationModel,\n",
    "    ClaudeQAModel,\n",
    ")\n",
    "\n",
    "# Use Claude for summarization + QA, SBert for embeddings\n",
    "config = RetrievalAugmentationConfig(\n",
    "    summarization_model=ClaudeSummarizationModel(),\n",
    "    qa_model=ClaudeQAModel(),\n",
    "    embedding_model=SBertEmbeddingModel(),\n",
    ")\n",
    "\n",
    "RA = RetrievalAugmentation(config=config)\n",
    "RA.add_documents(text)\n",
    "\n",
    "answer = RA.answer_question(question=\"How did Cinderella reach her happy ending?\")\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-header",
   "metadata": {},
   "source": [
    "## Benchmark: RAPTOR vs Flat Retrieval\n",
    "\n",
    "Compare RAPTOR's hierarchical tree retrieval against standard flat FAISS retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raptor import RaptorBenchmark\n",
    "\n",
    "benchmark = RaptorBenchmark()\n",
    "\n",
    "questions = [\n",
    "    \"How did Cinderella reach her happy ending?\",\n",
    "    \"What role did the birds play in the story?\",\n",
    "    \"How were the stepsisters punished?\",\n",
    "]\n",
    "\n",
    "report = benchmark.run(text, questions)\n",
    "print(report.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}